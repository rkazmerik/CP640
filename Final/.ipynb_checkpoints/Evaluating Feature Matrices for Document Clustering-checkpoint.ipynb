{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering: An Evaluation of Feature Matricies\n",
    "\n",
    "- Wilfrid Laurier University (Winter 2018)\n",
    "- CS640 - Introduction to Machine Learning\n",
    "- Ryan Kazmerik (175826410)\n",
    "\n",
    "## Overview\n",
    "{ Still need to write this section, providing a brief overview of the experiment }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "### 1. 20newsgroups\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics and has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "Let's import the 20newsgroups dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-newsgroups dataset:\n",
      "   Documents= 18846\n",
      "   Categories= 20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# ds1 = 20-newsgroups dataset\n",
    "ds1 = fetch_20newsgroups(subset='all', categories=None, shuffle=True,  \n",
    "                         random_state=1, remove=('footers','quotes'))\n",
    "\n",
    "# Extract the target classes\n",
    "ds1_labels = ds1.target\n",
    "\n",
    "print '20-newsgroups dataset:'\n",
    "print '   Documents=', len(ds1.data)\n",
    "print '   Categories=', len(ds1.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reuters-21578\n",
    "Reuters-21578 is commonly used collection for text clustering and  classification as it contains structured information about newswire articles that can be assigned to several classes, making it a multi-label problem.\n",
    "\n",
    "Let's import the reuters-21578 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters-21578 dataset:\n",
      "   Documents= 10788\n",
      "   Categories= 90\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "docs = reuters.fileids()\n",
    "\n",
    "# ds2 = Reuters-21578 dataset\n",
    "ds2_ids = list(docs)\n",
    "\n",
    "ds2 = [reuters.raw(doc_id) for doc_id in ds2_ids]\n",
    "\n",
    "# Extract the target classes\n",
    "mlb = MultiLabelBinarizer()\n",
    "ds2_labels = mlb.fit_transform([reuters.categories(doc_id)\n",
    "                                  for doc_id in ds2_ids])\n",
    " \n",
    "print 'Reuters-21578 dataset:'\n",
    "print '   Documents=', len(ds2)\n",
    "print '   Categories=', len(reuters.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Features\n",
    "The text in the documents must be parsed to remove stop words (tokenization) and the words need to be encoded as floating point values to be used as input for our clustering algorithm (vectorization).\n",
    "\n",
    "Let's create some feature vectors for our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-newsgroups features:\n",
      "   Num features: 10\n",
      "   Non-zero components: 3.28207577205\n",
      "\n",
      "Reuters-21578 features:\n",
      "   Num features: 10\n",
      "   TF Non-zero components: 3.77975528365\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Use TF-IDF vectorizer for Kmeans & PLSA\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
    "                     max_features=10, stop_words='english')\n",
    "\n",
    "# Use TF (term count) vectorizer for LDA\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                     max_features=10, stop_words='english')\n",
    "\n",
    "# Generate feature sets for fs1 (20newsgroups) and fs2 (reuters)\n",
    "fs1_idf = tfidf_vectorizer.fit_transform(ds1.data)\n",
    "fs1_tf = tf_vectorizer.fit_transform(ds1.data)\n",
    "\n",
    "fs2_idf = tfidf_vectorizer.fit_transform(ds2)\n",
    "fs2_tf = tf_vectorizer.fit_transform(ds2)\n",
    "\n",
    "print '20-newsgroups features:'\n",
    "print '   Num features:', fs1_idf.shape[1]\n",
    "print '   Non-zero components:', fs1_tf.nnz / float(fs1_tf.shape[0])\n",
    "print ''\n",
    "print 'Reuters-21578 features:'\n",
    "print '   Num features:', fs2_idf.shape[1]\n",
    "print '   TF Non-zero components:', fs2_tf.nnz / float(fs2_tf.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (Kmeans, LDA, PLSA)\n",
    "Three algorithms are used to cluster the results including a standard implementation of Kmeans, Non-negative Matrix Factorization is applied with the generalized Kullback-Leibler divergence which is equivalent to Probabilistic Latent Semantic Analysis (PLSA).\n",
    "\n",
    "Let's cluster our feature sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the kMeans model...\n",
      "   done in: 0.605268001556\n",
      "\n",
      "Fitting the LDA model...\n",
      "   done in: 44.9981751442\n",
      "\n",
      "Fitting the PLSA model...\n",
      "   done in: 7.85732388496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from time import time\n",
    "\n",
    "# Fit the kMeans model\n",
    "t0 = time()\n",
    "KM = KMeans(n_clusters=4, init='k-means++', max_iter=100, \n",
    "            n_init=1, verbose=0)\n",
    "print'Fitting the kMeans model...'\n",
    "\n",
    "fs1_KM = KM.fit(fs1_idf)\n",
    "fs2_KM = KM.fit(fs2_idf)\n",
    "\n",
    "print'   done in:', (time()-t0)\n",
    "print ''\n",
    "\n",
    "# Fit the LDA model\n",
    "t0 = time()\n",
    "LDA = LatentDirichletAllocation(n_components=10, max_iter=5,\n",
    "            learning_method='online', learning_offset=50., random_state=0)\n",
    "print 'Fitting the LDA model...'\n",
    "fs1_LDA = LDA.fit(fs1_tf)\n",
    "fs2_LDA = LDA.fit(fs2_tf)\n",
    "\n",
    "print'   done in:', (time()-t0)\n",
    "print ''\n",
    "\n",
    "# Fit the PLSA model\n",
    "t0 = time()\n",
    "PLSA = NMF(n_components=10, random_state=1, beta_loss='kullback-leibler', \n",
    "           solver='mu', max_iter=1000, alpha=.1, l1_ratio=.5)\n",
    "print 'Fitting the PLSA model...'\n",
    "\n",
    "fs1_PLSA = PLSA.fit(fs1_idf)\n",
    "fs2_PLSA = PLSA.fit(fs2_idf)\n",
    "\n",
    "print'   done in:', (time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.637\n",
      "Completeness: 0.755\n",
      "V-measure: 0.691\n",
      "Adjusted Rand-Index: 0.612\n",
      "Silhouette Coefficient: 0.043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(F2, km.labels_, sample_size=1000))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Cluster Top Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: god com keith atheism atheists people religion don sgi say\n",
      "Cluster 1: game ca team hockey games nhl play espn university season\n",
      "Cluster 2: com people cramer government optilink don state clinton gay just\n",
      "Cluster 3: space nasa access henry com moon gov shuttle orbit digex\n"
     ]
    }
   ],
   "source": [
    "original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(4):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
